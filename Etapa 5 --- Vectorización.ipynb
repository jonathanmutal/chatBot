{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Formas de caracterizar en contexto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Haremos los siguientes experimentos para ver la mejor forma de caracterizar el contexto:\n",
    "    - bolsa de palabras (unigramas, bigramas, subwords, con y sin stopwords) \n",
    "    - Agregado por mi: tfidf con normalizaci√≥n para poner algunos pesos en las palabras\n",
    "    - si hay interacci√≥n del usuario objetivo y c√≥mo se representa\n",
    "    - si hay respuestas\n",
    "    - si hay multimedia y de qu√© tipo\n",
    "__COMPLETAR__\n",
    "\n",
    "__PREGUNTAR:\n",
    "si hay interacci√≥n del usuario objetivo y c√≥mo se representa??. si hay respuestas??__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usaremos diferentes word embeddings, obtenidos de diferentes tipos de corpus: gen√©ricos o espec√≠ficos de lenguaje generado por usuario o espec√≠ficos del corpus del que estamos aprendiendo.\n",
    "    - Es decir, tres diferentes tipos de word embeddings:\n",
    "      - SBWCE\n",
    "      - con el mismo corpus de chats\n",
    "      - con un corpus de twitter\n",
    "      \n",
    "__BUSCAR CORPUS DE TWITTER__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cargando los datos de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('short_ans1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Acordandonos de los datos.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(df.context.isnull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idchat</th>\n",
       "      <th>context</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nonDef</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>&lt;Media omitted&gt;. Lee Esta Historia!          ....</td>\n",
       "      <td>üëèüëèüëèüëèüëè</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>üëèüëèüëèüëèüëè. ‚ú≥Whatsapp le comunica, . que debido a l...</td>\n",
       "      <td>Jajaja</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Jajaja. üê† Ella es MI AMIGA‚ù§üòª: Una peque√±a loca...</td>\n",
       "      <td>Dale!!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Dale!!!. Si puedo en casa.estamos muy apretado...</td>\n",
       "      <td>Ok!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>Ok!</td>\n",
       "      <td>üòò</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>üòò. Hola Isa Lamento lo de tu mam√° üò• Que descan...</td>\n",
       "      <td>Gracias!üòò</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>Gracias!üòò. &lt;Media omitted&gt;. Para mi amiga herm...</td>\n",
       "      <td>nonDef</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2</td>\n",
       "      <td>Hola. Te he enviado 0,90 cent via PayPal. .   ...</td>\n",
       "      <td>Virus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2</td>\n",
       "      <td>Virus. Reci√©n te diste cuenta?. M√°s virus no p...</td>\n",
       "      <td>Religi√≥n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2</td>\n",
       "      <td>Religi√≥n. Cthulhu existe?. Hablemos de Dios. Y...</td>\n",
       "      <td>Mejor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2</td>\n",
       "      <td>Mejor. No pienso estar en el cielp. Eso creo q...</td>\n",
       "      <td>Cielo*</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2</td>\n",
       "      <td>Cielo*. No me sigas leseando te lo advierto po...</td>\n",
       "      <td>Tmb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2</td>\n",
       "      <td>Tmb. Por que no tiene sentido que Dios sea amo...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2</td>\n",
       "      <td>2. En que parte de la Biblia lo dice?. En la p...</td>\n",
       "      <td>._.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2</td>\n",
       "      <td>._.. Explicame como funciona con la biblia. Pa...</td>\n",
       "      <td>Ok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2</td>\n",
       "      <td>Ok. No te lo niego. Nadie es santo no existe t...</td>\n",
       "      <td>Si</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2</td>\n",
       "      <td>Si. Eso es fanatismo</td>\n",
       "      <td>Si</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2</td>\n",
       "      <td>Si. Pues f√≠jate que estan los 10 mandamientos ...</td>\n",
       "      <td>Por?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2</td>\n",
       "      <td>Por?. Para vos todo lo que dice la biblia fue ...</td>\n",
       "      <td>:P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2</td>\n",
       "      <td>:P. Quien dice que Dios esta mas haya de la ci...</td>\n",
       "      <td>Nope</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2</td>\n",
       "      <td>Nope. Buenas Mati ‚úåüèº bienvenido a nuestro grup...</td>\n",
       "      <td>üòÇ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2</td>\n",
       "      <td>üòÇ. Buenos dias. No es el mismo por que para es...</td>\n",
       "      <td>Hola</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2</td>\n",
       "      <td>Hola. Buenos d√≠as sr. Francisco. Bienvenido al...</td>\n",
       "      <td>Fuckyourmother</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2</td>\n",
       "      <td>Fuckyourmother. Respeto por favor</td>\n",
       "      <td>Pamela</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2</td>\n",
       "      <td>Pamela. &lt;Media omitted&gt;</td>\n",
       "      <td>Chupamela</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2</td>\n",
       "      <td>Chupamela. Que pasa</td>\n",
       "      <td>Hola</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2</td>\n",
       "      <td>Hola. Chupamela pamela. Chupa** la pij*</td>\n",
       "      <td>S√°cala</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2</td>\n",
       "      <td>S√°cala</td>\n",
       "      <td>Comportence</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2</td>\n",
       "      <td>Comportence. &lt;Media omitted&gt;. Tu vieja se comp...</td>\n",
       "      <td>Oka</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25690</th>\n",
       "      <td>180</td>\n",
       "      <td>üëÜüèªüëÜüèª</td>\n",
       "      <td>üëåüèª</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25691</th>\n",
       "      <td>180</td>\n",
       "      <td>üëåüèª</td>\n",
       "      <td>Ok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25692</th>\n",
       "      <td>180</td>\n",
       "      <td>Ok. Entonces ma√±ana les mando material para tr...</td>\n",
       "      <td>Ok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25693</th>\n",
       "      <td>180</td>\n",
       "      <td>Ok</td>\n",
       "      <td>agusroldan27@hotmail.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25694</th>\n",
       "      <td>180</td>\n",
       "      <td>agusroldan27@hotmail.com. Muchas gracias profe...</td>\n",
       "      <td>Hola</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25695</th>\n",
       "      <td>180</td>\n",
       "      <td>Hola. Siiiii. 9,30. Bueno profe, llevamos las ...</td>\n",
       "      <td>Ok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25696</th>\n",
       "      <td>180</td>\n",
       "      <td>Ok. Ah√≠ fue, av√≠same si llega. Si llego! Graci...</td>\n",
       "      <td>üëçüèªüëçüèªüëçüèª</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25697</th>\n",
       "      <td>180</td>\n",
       "      <td>üëçüèªüëçüèªüëçüèª. Chicos ya est√°n las cosas en el grupo ...</td>\n",
       "      <td>ü§ôüèºü§ôüèºüòò</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25698</th>\n",
       "      <td>180</td>\n",
       "      <td>ü§ôüèºü§ôüèºüòò. Ese material hay q leer para ma√±ana??. ...</td>\n",
       "      <td>üò¨</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25699</th>\n",
       "      <td>180</td>\n",
       "      <td>üò¨. Vamos a hacer unas preguntas escritas y uno...</td>\n",
       "      <td>ü§î</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25700</th>\n",
       "      <td>180</td>\n",
       "      <td>ü§î. Nop. Hay que pedirle a plan los apuntes de ...</td>\n",
       "      <td>üëçüèª</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25701</th>\n",
       "      <td>180</td>\n",
       "      <td>üëçüèª. Ya los reviso y subo</td>\n",
       "      <td>Gracias</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25702</th>\n",
       "      <td>180</td>\n",
       "      <td>Gracias. Profe, cu√°les son los temas que vemos...</td>\n",
       "      <td>Sip</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25703</th>\n",
       "      <td>180</td>\n",
       "      <td>Sip. Genial gracias. Hola profe confirmamos el...</td>\n",
       "      <td>nonDef</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25704</th>\n",
       "      <td>181</td>\n",
       "      <td>Para que aguante mis idioteces ? Nadie ok!. &lt;M...</td>\n",
       "      <td>U.u</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25705</th>\n",
       "      <td>181</td>\n",
       "      <td>U.u. Lo perdono con la condici√≥n de que me hab...</td>\n",
       "      <td>üòíüòÇüòÇüòÇ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25706</th>\n",
       "      <td>181</td>\n",
       "      <td>üòíüòÇüòÇüòÇ. Dicho y hecho me arrepenti. üòÇüòí</td>\n",
       "      <td>Holaa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25707</th>\n",
       "      <td>181</td>\n",
       "      <td>Holaa</td>\n",
       "      <td>Holaaa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25708</th>\n",
       "      <td>181</td>\n",
       "      <td>Holaaa</td>\n",
       "      <td>üë©‚Äçüé§</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25709</th>\n",
       "      <td>181</td>\n",
       "      <td>üë©‚Äçüé§</td>\n",
       "      <td>Hola</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25710</th>\n",
       "      <td>181</td>\n",
       "      <td>Hola. Que hacen?. Yo nada y ustedes</td>\n",
       "      <td>Nada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25711</th>\n",
       "      <td>181</td>\n",
       "      <td>Nada. :¬Æ</td>\n",
       "      <td>Hola..</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25712</th>\n",
       "      <td>181</td>\n",
       "      <td>Hola..</td>\n",
       "      <td>:c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25713</th>\n",
       "      <td>181</td>\n",
       "      <td>:c. Yo tumbado en cama agusto</td>\n",
       "      <td>Jajajaja</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25714</th>\n",
       "      <td>181</td>\n",
       "      <td>Jajajaja</td>\n",
       "      <td>ü§∑üèª‚Äç‚ôÄ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25715</th>\n",
       "      <td>181</td>\n",
       "      <td>ü§∑üèª‚Äç‚ôÄ. Alguien lee libros?. Me interesar√≠a much...</td>\n",
       "      <td>Hola</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25716</th>\n",
       "      <td>181</td>\n",
       "      <td>Hola. Me llamo Andrea. Bienvenida Andrea</td>\n",
       "      <td>Gracias</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25717</th>\n",
       "      <td>181</td>\n",
       "      <td>Gracias</td>\n",
       "      <td>üòâ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25718</th>\n",
       "      <td>181</td>\n",
       "      <td>üòâ. &lt;Media omitted&gt;. Qu√© pasa cabros. Qu√© estan...</td>\n",
       "      <td>ü§∑‚Äç‚ôÄ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25719</th>\n",
       "      <td>181</td>\n",
       "      <td>ü§∑‚Äç‚ôÄ. &lt;Media omitted&gt;. &lt;Media omitted&gt;. &lt;Media ...</td>\n",
       "      <td>ü§£ü§£</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25720 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       idchat                                            context  \\\n",
       "0           0                                                NaN   \n",
       "1           1  <Media omitted>. Lee Esta Historia!          ....   \n",
       "2           1  üëèüëèüëèüëèüëè. ‚ú≥Whatsapp le comunica, . que debido a l...   \n",
       "3           1  Jajaja. üê† Ella es MI AMIGA‚ù§üòª: Una peque√±a loca...   \n",
       "4           1  Dale!!!. Si puedo en casa.estamos muy apretado...   \n",
       "5           1                                                Ok!   \n",
       "6           1  üòò. Hola Isa Lamento lo de tu mam√° üò• Que descan...   \n",
       "7           1  Gracias!üòò. <Media omitted>. Para mi amiga herm...   \n",
       "8           2  Hola. Te he enviado 0,90 cent via PayPal. .   ...   \n",
       "9           2  Virus. Reci√©n te diste cuenta?. M√°s virus no p...   \n",
       "10          2  Religi√≥n. Cthulhu existe?. Hablemos de Dios. Y...   \n",
       "11          2  Mejor. No pienso estar en el cielp. Eso creo q...   \n",
       "12          2  Cielo*. No me sigas leseando te lo advierto po...   \n",
       "13          2  Tmb. Por que no tiene sentido que Dios sea amo...   \n",
       "14          2  2. En que parte de la Biblia lo dice?. En la p...   \n",
       "15          2  ._.. Explicame como funciona con la biblia. Pa...   \n",
       "16          2  Ok. No te lo niego. Nadie es santo no existe t...   \n",
       "17          2                               Si. Eso es fanatismo   \n",
       "18          2  Si. Pues f√≠jate que estan los 10 mandamientos ...   \n",
       "19          2  Por?. Para vos todo lo que dice la biblia fue ...   \n",
       "20          2  :P. Quien dice que Dios esta mas haya de la ci...   \n",
       "21          2  Nope. Buenas Mati ‚úåüèº bienvenido a nuestro grup...   \n",
       "22          2  üòÇ. Buenos dias. No es el mismo por que para es...   \n",
       "23          2  Hola. Buenos d√≠as sr. Francisco. Bienvenido al...   \n",
       "24          2                  Fuckyourmother. Respeto por favor   \n",
       "25          2                            Pamela. <Media omitted>   \n",
       "26          2                                Chupamela. Que pasa   \n",
       "27          2            Hola. Chupamela pamela. Chupa** la pij*   \n",
       "28          2                                             S√°cala   \n",
       "29          2  Comportence. <Media omitted>. Tu vieja se comp...   \n",
       "...       ...                                                ...   \n",
       "25690     180                                               üëÜüèªüëÜüèª   \n",
       "25691     180                                                 üëåüèª   \n",
       "25692     180  Ok. Entonces ma√±ana les mando material para tr...   \n",
       "25693     180                                                 Ok   \n",
       "25694     180  agusroldan27@hotmail.com. Muchas gracias profe...   \n",
       "25695     180  Hola. Siiiii. 9,30. Bueno profe, llevamos las ...   \n",
       "25696     180  Ok. Ah√≠ fue, av√≠same si llega. Si llego! Graci...   \n",
       "25697     180  üëçüèªüëçüèªüëçüèª. Chicos ya est√°n las cosas en el grupo ...   \n",
       "25698     180  ü§ôüèºü§ôüèºüòò. Ese material hay q leer para ma√±ana??. ...   \n",
       "25699     180  üò¨. Vamos a hacer unas preguntas escritas y uno...   \n",
       "25700     180  ü§î. Nop. Hay que pedirle a plan los apuntes de ...   \n",
       "25701     180                           üëçüèª. Ya los reviso y subo   \n",
       "25702     180  Gracias. Profe, cu√°les son los temas que vemos...   \n",
       "25703     180  Sip. Genial gracias. Hola profe confirmamos el...   \n",
       "25704     181  Para que aguante mis idioteces ? Nadie ok!. <M...   \n",
       "25705     181  U.u. Lo perdono con la condici√≥n de que me hab...   \n",
       "25706     181               üòíüòÇüòÇüòÇ. Dicho y hecho me arrepenti. üòÇüòí   \n",
       "25707     181                                              Holaa   \n",
       "25708     181                                             Holaaa   \n",
       "25709     181                                                üë©‚Äçüé§   \n",
       "25710     181                Hola. Que hacen?. Yo nada y ustedes   \n",
       "25711     181                                           Nada. :¬Æ   \n",
       "25712     181                                             Hola..   \n",
       "25713     181                      :c. Yo tumbado en cama agusto   \n",
       "25714     181                                           Jajajaja   \n",
       "25715     181  ü§∑üèª‚Äç‚ôÄ. Alguien lee libros?. Me interesar√≠a much...   \n",
       "25716     181           Hola. Me llamo Andrea. Bienvenida Andrea   \n",
       "25717     181                                            Gracias   \n",
       "25718     181  üòâ. <Media omitted>. Qu√© pasa cabros. Qu√© estan...   \n",
       "25719     181  ü§∑‚Äç‚ôÄ. <Media omitted>. <Media omitted>. <Media ...   \n",
       "\n",
       "                          label  \n",
       "0                        nonDef  \n",
       "1                         üëèüëèüëèüëèüëè  \n",
       "2                        Jajaja  \n",
       "3                       Dale!!!  \n",
       "4                           Ok!  \n",
       "5                             üòò  \n",
       "6                     Gracias!üòò  \n",
       "7                        nonDef  \n",
       "8                         Virus  \n",
       "9                      Religi√≥n  \n",
       "10                        Mejor  \n",
       "11                       Cielo*  \n",
       "12                          Tmb  \n",
       "13                            2  \n",
       "14                          ._.  \n",
       "15                           Ok  \n",
       "16                           Si  \n",
       "17                           Si  \n",
       "18                         Por?  \n",
       "19                           :P  \n",
       "20                         Nope  \n",
       "21                            üòÇ  \n",
       "22                         Hola  \n",
       "23               Fuckyourmother  \n",
       "24                       Pamela  \n",
       "25                    Chupamela  \n",
       "26                         Hola  \n",
       "27                       S√°cala  \n",
       "28                  Comportence  \n",
       "29                          Oka  \n",
       "...                         ...  \n",
       "25690                        üëåüèª  \n",
       "25691                        Ok  \n",
       "25692                        Ok  \n",
       "25693  agusroldan27@hotmail.com  \n",
       "25694                      Hola  \n",
       "25695                        Ok  \n",
       "25696                    üëçüèªüëçüèªüëçüèª  \n",
       "25697                     ü§ôüèºü§ôüèºüòò  \n",
       "25698                         üò¨  \n",
       "25699                         ü§î  \n",
       "25700                        üëçüèª  \n",
       "25701                   Gracias  \n",
       "25702                       Sip  \n",
       "25703                    nonDef  \n",
       "25704                       U.u  \n",
       "25705                      üòíüòÇüòÇüòÇ  \n",
       "25706                     Holaa  \n",
       "25707                    Holaaa  \n",
       "25708                       üë©‚Äçüé§  \n",
       "25709                      Hola  \n",
       "25710                      Nada  \n",
       "25711                    Hola..  \n",
       "25712                        :c  \n",
       "25713                  Jajajaja  \n",
       "25714                      ü§∑üèª‚Äç‚ôÄ  \n",
       "25715                      Hola  \n",
       "25716                   Gracias  \n",
       "25717                         üòâ  \n",
       "25718                       ü§∑‚Äç‚ôÄ  \n",
       "25719                        ü§£ü§£  \n",
       "\n",
       "[25720 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorizando"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BOG: 1-gramas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(min_df=10, ngram_range=(1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "representation = vectorizer.fit(df[1:].context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entonces obtenemos alrededor de tantas features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['MEDIA', 'NUM', 'aNUM', 'aa', 'aah', 'aaja', 'abajo', 'abi', 'abierta', 'abiertas', 'abierto', 'abra', 'abran', 'abrazo', 'abrazos', 'abre', 'abren', 'abri', 'abril', 'abrir', 'abro', 'abuela', 'abuelo', 'aburrido', 'abuso', 'aca', 'acaba', 'acaban', 'acabar', 'acabaron', 'acabas', 'acabe', 'acabo', 'academia', 'acaso', 'acceder', 'acceso', 'accion', 'acepta', 'acepte', 'acepto', 'acerca', 'aclarar', 'aclaro', 'acompana', 'acompanar', 'acompano', 'acordaba', 'acordar', 'acordas', 'acordate', 'acorde', 'actitud', 'activa', 'activan', 'activar', 'active', 'actividad', 'actividades', 'activo', 'activos', 'acto', 'actores', 'actual', 'actualiza', 'actualizacion', 'actualizar', 'actualizo', 'acuerda', 'acuerdan', 'acuerdo', 'acv', 'add', 'adelantar', 'adelante', 'adelanto', 'ademas', 'adentro', 'adios', 'adjuntado', 'admin', 'administrador', 'adri', 'adrian', 'ae', 'aeropuerto', 'afecta', 'afiche', 'after', 'afuera', 'again', 'agarra', 'agarrar', 'agarre', 'agarro', 'age', 'agenda', 'agendo', 'ago', 'agosto']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5323"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(representation.get_feature_names()[:100])\n",
    "len(representation.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOG = representation.transform(df[1:].context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 0)\t1\n",
      "  (0, 1)\t4\n",
      "  (0, 11)\t1\n",
      "  (0, 41)\t1\n",
      "  (0, 117)\t2\n",
      "  (0, 128)\t1\n",
      "  (0, 145)\t2\n",
      "  (0, 163)\t2\n",
      "  (0, 215)\t2\n",
      "  (0, 251)\t1\n",
      "  (0, 350)\t2\n",
      "  (0, 425)\t1\n",
      "  (0, 495)\t1\n",
      "  (0, 511)\t2\n",
      "  (0, 623)\t1\n",
      "  (0, 741)\t1\n",
      "  (0, 829)\t1\n",
      "  (0, 930)\t2\n",
      "  (0, 980)\t1\n",
      "  (0, 1057)\t1\n",
      "  (0, 1131)\t1\n",
      "  (0, 1155)\t2\n",
      "  (0, 1230)\t1\n",
      "  (0, 1258)\t7\n",
      "  (0, 1278)\t1\n",
      "  :\t:\n",
      "  (2, 4589)\t1\n",
      "  (2, 4639)\t1\n",
      "  (2, 4742)\t1\n",
      "  (2, 4778)\t1\n",
      "  (2, 4840)\t1\n",
      "  (2, 4847)\t1\n",
      "  (2, 4969)\t1\n",
      "  (2, 5010)\t1\n",
      "  (2, 5011)\t1\n",
      "  (2, 5240)\t1\n",
      "  (2, 5274)\t1\n",
      "  (2, 5300)\t1\n",
      "  (2, 5309)\t1\n",
      "  (3, 350)\t1\n",
      "  (3, 738)\t1\n",
      "  (3, 1221)\t1\n",
      "  (3, 1258)\t1\n",
      "  (3, 1639)\t1\n",
      "  (3, 1812)\t1\n",
      "  (3, 3052)\t1\n",
      "  (3, 3256)\t1\n",
      "  (3, 3810)\t1\n",
      "  (3, 4062)\t1\n",
      "  (3, 4542)\t1\n",
      "  (4, 3428)\t1\n"
     ]
    }
   ],
   "source": [
    "print(BOG[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "save(\"BOG1.pickle\", BOG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BOG: 2-gramas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=10,\n",
       "        ngram_range=(1, 2),\n",
       "        preprocessor=<function clean_text_1 at 0x7ff73bdf0950>,\n",
       "        stop_words=None, strip_accents=None,\n",
       "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "representation.set_params(ngram_range=(1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14401"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BOG = representation.fit(df[1:].context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BOG: 3-gramas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=10,\n",
       "        ngram_range=(1, 3),\n",
       "        preprocessor=<function clean_text_1 at 0x7fafab945c80>,\n",
       "        stop_words=None, strip_accents=None,\n",
       "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "representation.set_params(ngram_range=(1, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17048"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(representation.fit(df[1:].context).get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOG_matrix = representation.transform(df[1:].context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(56778, 17048)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BOG_matrix.get_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'chicos. una pregunta. c√≥mo se escribe la direcci√≥n postal en vuestras ciudades?. es que los otros d√≠as vi una de francia y es un poco diferente a como yo escribo la m√≠a. La de aca por donde vivo es 1010. El cod postal. as√≠ que si pongo n un sobre 1010 te llega?. Pero es por zona, no se si s igual por alla. qu√© raro sois los venezolanos. no?. No, obvio pones la direccionüòÇüòÇ'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.context[28]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ NUM: 2, aca: 1, aca por: 1, alla: 1, asi: 1, asi que: 1, asi que si: 1, chicos: 1, chicos una: 1, ciudades: 1, cod: 1, como: 2, como se: 1, como yo: 1, de: 2, de aca: 1, dias: 1, diferente: 1, direccion: 2, donde: 1, en: 1, es: 4, es NUM: 1, es por: 1, es que: 1, es un: 1, es un poco: 1, escribe: 1, escribo: 1, igual: 1, igual por: 1, la: 4, la de: 1, la direccion: 2, la mia: 1, llega: 1, los: 2, los otros: 1, mia: 1, no: 3, no no: 1, no se: 1, no se si: 1, obvio: 1, otros: 1, pero: 1, pero es: 1, poco: 1, pones: 1, pongo: 1, por: 3, por alla: 1, por donde: 1, pregunta: 1, que: 3, que los: 1, que raro: 1, que si: 1, raro: 1, se: 2, se escribe: 1, se si: 1, si: 2, si igual: 1, sobre: 1, un: 2, un poco: 1, una: 2, una de: 1, una pregunta: 1, vi: 1, vivo: 1, yo: 1, zona: 1, }"
     ]
    }
   ],
   "source": [
    "arr_repr = BOG_matrix[27].toarray()[0]\n",
    "print('{ ', end='')\n",
    "for i in BOG_matrix[27].indices:\n",
    "    print('{0}: {1}'.format(representation.get_feature_names()[i], arr_repr[i]), end=', ')\n",
    "print('}', end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BOG: (3,4)-Subwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(min_df=10, analyzer='char', ngram_range=(3,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='char', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=10,\n",
       "        ngram_range=(3, 4),\n",
       "        preprocessor=<function clean_text_1 at 0x7fafab945c80>,\n",
       "        stop_words='!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~¬°',\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "representation = vectorizer.fit(df[1:].context)\n",
    "BOG_matrix = representation.transform(df[1:].context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24863"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(representation.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'chicos. una pregunta. c√≥mo se escribe la direcci√≥n postal en vuestras ciudades?. es que los otros d√≠as vi una de francia y es un poco diferente a como yo escribo la m√≠a. La de aca por donde vivo es 1010. El cod postal. as√≠ que si pongo n un sobre 1010 te llega?. Pero es por zona, no se si s igual por alla. qu√© raro sois los venezolanos. no?. No, obvio pones la direccionüòÇüòÇ'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.context[28]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " NU: 2,  NUM: 2,  a : 1,  a c: 1,  ac: 1,  aca: 1,  al: 1,  all: 1,  as: 1,  asi: 1,  ci: 1,  ciu: 1,  co: 3,  cod: 1,  com: 2,  de: 2,  de : 2,  di: 4,  dia: 1,  dif: 1,  dir: 2,  do: 1,  don: 1,  en: 1,  en : 1,  es: 6,  es : 4,  esc: 2,  fr: 1,  fra: 1,  ig: 1,  igu: 1,  la: 4,  la : 4,  ll: 1,  lle: 1,  lo: 2,  los: 2,  mi: 1,  mia: 1,  n : 1,  no: 3,  no : 3,  ob: 1,  obv: 1,  ot: 1,  otr: 1,  pe: 1,  per: 1,  po: 8,  poc: 1,  pon: 2,  por: 3,  pos: 2,  pr: 1,  pre: 1,  qu: 3,  que: 3,  ra: 1,  rar: 1,  s : 1,  se: 2,  se : 2,  si: 2,  si : 2,  so: 2,  sob: 1,  un: 4,  un : 2,  una: 2,  ve: 1,  ven: 1,  vi: 2,  vi : 1,  viv: 1,  vu: 1,  vue: 1,  y : 1,  y e: 1,  yo: 1,  yo : 1,  zo: 1,  zon: 1, M c: 1, M co: 1, M l: 1, M ll: 1, NUM: 2, NUM : 2, UM : 2, UM c: 1, UM l: 1, a c: 2, a co: 2, a d: 4, a de: 2, a di: 2, a l: 1, a la: 1, a m: 1, a mi: 1, a n: 1, a no: 1, a p: 3, a pe: 1, a po: 1, a pr: 1, a q: 1, a qu: 1, a y: 1, a y : 1, aca: 1, aca : 1, ade: 1, ades: 1, al : 3, al a: 1, al e: 1, al p: 1, all: 1, alla: 1, anc: 1, anci: 1, ano: 1, anos: 1, aro: 1, aro : 1, as : 2, as c: 1, as v: 1, asi: 1, asi : 1, be : 1, be l: 1, bo : 1, bo l: 1, bre: 1, bre : 1, bvi: 1, bvio: 1, ca : 1, ca p: 1, cci: 2, ccio: 2, chi: 1, chic: 1, cia: 1, cia : 1, cio: 2, cion: 2, ciu: 1, ciud: 1, co : 1, co d: 1, cod: 1, cod : 1, com: 2, como: 2, cos: 1, cos : 1, cri: 2, crib: 2, d p: 1, d po: 1, dad: 1, dade: 1, de : 3, de a: 1, de f: 1, de v: 1, des: 1, des : 1, dia: 1, dias: 1, dif: 1, dife: 1, dir: 2, dire: 2, don: 1, dond: 1, e N: 1, e NU: 1, e a: 2, e a : 1, e ac: 1, e e: 1, e es: 1, e f: 1, e fr: 1, e l: 2, e la: 1, e lo: 1, e r: 1, e ra: 1, e s: 2, e si: 2, e v: 1, e vi: 1, ecc: 2, ecci: 2, ega: 1, ega : 1, egu: 1, egun: 1, en : 1, en v: 1, ene: 1, enez: 1, ent: 1, ente: 1, ere: 1, eren: 1, ero: 1, ero : 1, es : 6, es N: 1, es e: 1, es l: 1, es p: 1, es q: 1, es u: 1, esc: 2, escr: 2, est: 1, estr: 1, ezo: 1, fer: 1, fere: 1, fra: 1, fran: 1, ga : 1, ga p: 1, go : 1, go n: 1, gua: 1, gual: 1, gun: 1, gunt: 1, hic: 1, hico: 1, i p: 1, i po: 1, i q: 1, i qu: 1, i s: 1, i u: 1, i un: 1, ia : 2, ia l: 1, ia y: 1, ias: 1, ias : 1, ibe: 1, ibe : 1, ibo: 1, ibo : 1, ico: 1, icos: 1, ife: 1, ifer: 1, igu: 1, igua: 1, io : 1, io p: 1, ion: 2, ion : 1, ire: 2, irec: 2, is : 1, is l: 1, iud: 1, iuda: 1, ivo: 1, ivo : 1, l a: 1, l as: 1, l e: 1, l en: 1, l p: 1, l po: 1, la : 5, la d: 3, la m: 1, la q: 1, lan: 1, lano: 1, leg: 1, lega: 1, lla: 1, lla : 1, lle: 1, lleg: 1, los: 2, los : 2, mia: 1, mia : 1, mo : 2, mo s: 1, mo y: 1, n p: 2, n po: 2, n s: 1, n so: 1, n u: 1, n un: 1, n v: 1, n vu: 1, na : 3, na d: 1, na n: 1, na p: 1, nci: 1, ncia: 1, nde: 1, nde : 1, nes: 1, nes : 1, nez: 1, ngo: 1, ngo : 1, no : 3, no n: 1, no o: 1, no s: 1, nos: 1, nos : 1, nta: 1, nta : 1, nte: 1, nte : 1, o d: 1, o di: 1, o e: 3, o es: 3, o l: 1, o la: 1, o n: 2, o n : 1, o no: 1, o o: 1, o ob: 1, o p: 1, o po: 1, o s: 3, o se: 2, o so: 1, o y: 1, o yo: 1, obr: 1, obre: 1, obv: 1, obvi: 1, oco: 1, oco : 1, od : 1, od p: 1, ois: 1, ois : 1, ola: 1, olan: 1, omo: 2, omo : 2, on : 1, on p: 1, ona: 1, ona : 1, ond: 1, onde: 1, one: 1, ones: 1, ong: 1, ongo: 1, or : 3, or a: 1, or d: 1, os : 5, os d: 1, os n: 1, os o: 1, os u: 1, os v: 1, ost: 2, osta: 2, otr: 1, otro: 1, per: 1, pero: 1, poc: 1, poco: 1, pon: 2, pone: 1, pong: 1, por: 3, por : 3, pos: 2, post: 2, pre: 1, preg: 1, que: 3, que : 3, r a: 1, r al: 1, r d: 1, r do: 1, r z: 1, ran: 1, ranc: 1, rar: 1, raro: 1, ras: 1, ras : 1, re : 1, re N: 1, rec: 2, recc: 2, reg: 1, regu: 1, ren: 1, rent: 1, rib: 2, ribe: 1, ribo: 1, ro : 2, ro e: 1, ro s: 1, ros: 1, ros : 1, s N: 1, s NU: 1, s c: 1, s ci: 1, s d: 1, s di: 1, s e: 1, s es: 1, s i: 1, s ig: 1, s l: 2, s la: 1, s lo: 1, s n: 1, s no: 1, s o: 1, s ot: 1, s p: 1, s po: 1, s q: 1, s qu: 1, s u: 2, s un: 2, s v: 2, s ve: 1, s vi: 1, scr: 2, scri: 2, se : 2, se e: 1, se s: 1, si : 3, si p: 1, si q: 1, si s: 1, sob: 1, sobr: 1, soi: 1, sta: 2, stal: 2, str: 1, stra: 1, ta : 1, ta c: 1, tal: 2, tal : 2, te : 1, te a: 1, tra: 1, tras: 1, tro: 1, tros: 1, ual: 1, ual : 1, uda: 1, udad: 1, ue : 3, ue l: 1, ue r: 1, ue s: 1, ues: 1, uest: 1, un : 2, un p: 1, un s: 1, una: 2, una : 2, unt: 1, unta: 1, ven: 1, vene: 1, vi : 1, vi u: 1, vio: 1, vio : 1, viv: 1, vivo: 1, vo : 1, vo e: 1, vue: 1, y e: 1, y es: 1, yo : 1, yo e: 1, zon: 1, zona: 1, "
     ]
    }
   ],
   "source": [
    "arr_repr = BOG_matrix[27].toarray()[0]\n",
    "for i in BOG_matrix[27].indices:\n",
    "    print('{0}: {1}'.format(representation.get_feature_names()[i], arr_repr[i]), end=', ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFidf: 1-gramas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(min_df=10, ngram_range=(1, 1), norm='l2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "representation = vectorizer.fit(df[1:].context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOG_matrix = representation.transform(df[1:].context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entonces podemos decir que la frase:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'üòò. Hola Isa Lamento lo de tu mam√° üò• Que descanse en paz'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.context[7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta representada como:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 4610)\t0.444007229186\n",
      "  (0, 4608)\t0.232292394347\n",
      "  (0, 3939)\t0.285705626552\n",
      "  (0, 3473)\t0.497750374311\n",
      "  (0, 3381)\t0.647699532027\n"
     ]
    }
   ],
   "source": [
    "print(BOG_matrix[6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Donde los features son..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('saca', 'sabor', 'peso', 'necesiten', 'moral')"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "representation.get_feature_names()[4449], representation.get_feature_names()[4447], representation.get_feature_names()[3802], representation.get_feature_names()[3348], representation.get_feature_names()[3261]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFidf: 3-gramas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(min_df=10, ngram_range=(1, 3), norm='l2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "representation = vectorizer.fit(df[1:].context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOG_matrix = representation.transform(df[1:].context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 14962)\t0.251093106197\n",
      "  (0, 10991)\t0.526693785555\n",
      "  (0, 7090)\t0.708823214338\n",
      "  (0, 6946)\t0.34980486296\n",
      "  (0, 3044)\t0.186419196821\n",
      "  (1, 13416)\t0.445651009621\n",
      "  (1, 12802)\t0.260870206023\n",
      "  (1, 12700)\t0.114103432201\n",
      "  (1, 9129)\t0.312658204155\n",
      "  (1, 6600)\t0.238068586996\n",
      "  (1, 5707)\t0.481824210078\n",
      "  (1, 5161)\t0.445651009621\n",
      "  (1, 5101)\t0.145516231116\n",
      "  (1, 1117)\t0.337573917747\n",
      "  (2, 16328)\t0.45418585109\n",
      "  (2, 11596)\t0.520149510929\n",
      "  (2, 7360)\t0.463337136898\n",
      "  (2, 6171)\t0.360458478114\n",
      "  (2, 1504)\t0.422549502516\n",
      "  (3, 10995)\t0.391758851566\n",
      "  (3, 10264)\t0.471306759228\n",
      "  (3, 7671)\t0.549778867213\n",
      "  (3, 4420)\t0.514157183574\n",
      "  (3, 4180)\t0.175496199971\n",
      "  (3, 3044)\t0.164260806241\n",
      "  (6, 14494)\t0.354404835023\n",
      "  (6, 14225)\t0.185414881299\n",
      "  (6, 12162)\t0.22804911449\n",
      "  (6, 11046)\t0.602397708492\n",
      "  (6, 11040)\t0.397302403417\n",
      "  (6, 10463)\t0.51699123506\n",
      "  (7, 13366)\t1.0\n",
      "  (8, 16875)\t1.0\n"
     ]
    }
   ],
   "source": [
    "print(BOG_matrix[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entonces podemos decir que la frase:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'chicos. una pregunta. c√≥mo se escribe la direcci√≥n postal en vuestras ciudades?. es que los otros d√≠as vi una de francia y es un poco diferente a como yo escribo la m√≠a. La de aca por donde vivo es 1010. El cod postal. as√≠ que si pongo n un sobre 1010 te llega?. Pero es por zona, no se si s igual por alla. qu√© raro sois los venezolanos. no?. No, obvio pones la direccionüòÇüòÇ'"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.context[28]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta representada como:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 17044)\t0.122346591758\n",
      "  (0, 16875)\t0.0505344426973\n",
      "  (0, 16562)\t0.11856716976\n",
      "  (0, 16448)\t0.0876481532616\n",
      "  (0, 16023)\t0.114368515018\n",
      "  (0, 15958)\t0.111328285247\n",
      "  (0, 15924)\t0.109401534642\n",
      "  (0, 15876)\t0.09219704563\n",
      "  (0, 15780)\t0.10054062254\n",
      "  (0, 14590)\t0.0924285941421\n",
      "  (0, 14324)\t0.128110828987\n",
      "  (0, 14225)\t0.0864893830059\n",
      "  (0, 14003)\t0.092006694011\n",
      "  (0, 13890)\t0.139659678239\n",
      "  (0, 13857)\t0.0964611674483\n",
      "  (0, 13438)\t0.10145416588\n",
      "  (0, 13135)\t0.0891153203862\n",
      "  (0, 13097)\t0.123745147403\n",
      "  (0, 12956)\t0.104691433598\n",
      "  (0, 12700)\t0.108599777491\n",
      "  (0, 12378)\t0.0970086224212\n",
      "  (0, 12181)\t0.11697464841\n",
      "  (0, 12173)\t0.142325670723\n",
      "  (0, 12162)\t0.159565082391\n",
      "  (0, 12151)\t0.0983157920864\n",
      "  :\t:\n",
      "  (0, 5245)\t0.0841350833944\n",
      "  (0, 5237)\t0.113895987251\n",
      "  (0, 5102)\t0.11420940119\n",
      "  (0, 5101)\t0.184663219042\n",
      "  (0, 4632)\t0.043493175501\n",
      "  (0, 4027)\t0.0763201425025\n",
      "  (0, 3957)\t0.242623955266\n",
      "  (0, 3875)\t0.128906565576\n",
      "  (0, 3822)\t0.0942850049113\n",
      "  (0, 3058)\t0.120578921785\n",
      "  (0, 3044)\t0.0748627773091\n",
      "  (0, 2302)\t0.131095183094\n",
      "  (0, 2277)\t0.100379210455\n",
      "  (0, 2216)\t0.111830969225\n",
      "  (0, 2157)\t0.13886394165\n",
      "  (0, 2092)\t0.145546876183\n",
      "  (0, 2038)\t0.142325670723\n",
      "  (0, 2015)\t0.0806051631679\n",
      "  (0, 1217)\t0.136695750024\n",
      "  (0, 1212)\t0.0879948372376\n",
      "  (0, 1181)\t0.0641662197014\n",
      "  (0, 924)\t0.0966983428321\n",
      "  (0, 404)\t0.145546876183\n",
      "  (0, 389)\t0.0818514394602\n",
      "  (0, 158)\t0.0912586035071\n"
     ]
    }
   ],
   "source": [
    "print(BOG_matrix[27])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Donde los features son..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zona: 0.12234659175774053, yo: 0.050534442697347366, vivo: 0.11856716975995497, vi: 0.0876481532615862, una pregunta: 0.1143685150183527, una de: 0.11132828524748828, una: 0.10940153464161766, un poco: 0.09219704562998841, un: 0.10054062254010157, sobre: 0.09242859414211325, si igual: 0.1281108289871265, si: 0.0864893830059329, se si: 0.09200669401099533, se escribe: 0.13965967823906839, se: 0.09646116744828899, raro: 0.1014541658802468, que si: 0.0891153203862292, que raro: 0.12374514740338534, que los: 0.10469143359780607, que: 0.10859977749099453, pregunta: 0.09700862242124296, por donde: 0.11697464840972999, por alla: 0.14232567072328067, por: 0.15956508239073838, pongo: 0.09831579208638877, pones: 0.11587912544216117, poco: 0.085767058832565, pero es: 0.10377616506189986, pero: 0.05238518845298545, otros: 0.09759410557706343, obvio: 0.09968114551481815, no se si: 0.09274280158788244, no se: 0.06596438580282649, no no: 0.09500251833050471, no: 0.11351884900093571, mia: 0.11132828524748828, los otros: 0.11420940118955161, los: 0.10530462068028322, llega: 0.11199996861179086, la mia: 0.12182066131348711, la direccion: 0.264130180590944, la de: 0.10057517258490781, la: 0.15812530743968584, igual por: 0.13965967823906839, igual: 0.07116729752801136, escribo: 0.11502180995925212, escribe: 0.12345516770236246, es un poco: 0.14049844803858233, es un: 0.08643941409042437, es que: 0.08413508339439471, es por: 0.11389598725097232, es NUM: 0.11420940118955161, es: 0.184663219042431, en: 0.0434931755009734, donde: 0.07632014250248814, direccion: 0.24262395526578306, diferente: 0.1289065655764879, dias: 0.09428500491126841, de aca: 0.12057892178460922, de: 0.07486277730914422, como yo: 0.1310951830936827, como se: 0.10037921045534302, como: 0.111830969225467, cod: 0.13886394164970695, ciudades: 0.1455468761833439, chicos una: 0.14232567072328067, chicos: 0.08060516316789397, asi que si: 0.13669575002395862, asi que: 0.087994837237629, asi: 0.06416621970135543, alla: 0.09669834283206566, aca por: 0.1455468761833439, aca: 0.08185143946016502, NUM: 0.09125860350705094, "
     ]
    }
   ],
   "source": [
    "dict_repr = dict()\n",
    "arr_repr = BOG_matrix[27].toarray()[0]\n",
    "for i in BOG_matrix[27].indices:\n",
    "    print('{0}: {1}'.format(representation.get_feature_names()[i], arr_repr[i]), end=', ')\n",
    "    dict_repr[representation.get_feature_names()[i]] = arr_repr[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Palabras m√°s representativas para la frase:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('la direccion', 0.26413018059094401),\n",
       " ('direccion', 0.24262395526578306),\n",
       " ('es', 0.18466321904243099),\n",
       " ('por', 0.15956508239073838),\n",
       " ('la', 0.15812530743968584),\n",
       " ('ciudades', 0.14554687618334389),\n",
       " ('aca por', 0.14554687618334389),\n",
       " ('por alla', 0.14232567072328067),\n",
       " ('chicos una', 0.14232567072328067),\n",
       " ('es un poco', 0.14049844803858233)]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(dict_repr.items(), key=lambda x: x[1], reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "filename = \"BOGmatrix.pickle\"\n",
    "fileObj = open(filename, 'wb')\n",
    "pickle.dump(ctx, fileObj)\n",
    "fileObj.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pues claramente esta pidiendo una direcci√≥n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tomamos algunos embeddings de:\n",
    "- [SBWCE](http://crscardellino.me/SBWCE/) --> 300 dimensiones\n",
    "- Nuestro corupus --> 100 dimensiones\n",
    "\n",
    "**TWITER???**\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SBWCE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Empezaremos cargando los vectores pre-entrenados con el algoritmo de Word2Vec. Para ello usaremos Gensim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KeyedVectors.load_word2vec_format('embeddings/SBW-vectors-300-min5.bin.gz', binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De esta forma no nos alcanza la memoria (por ahora), por lo que vamos a filtrar las palabras que usaremos y guardaremos un nuevo archivo m√°s peque√±o."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Achicando el vocabulario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "distribution_words = Counter(chain.from_iterable(df_cleaned.map(lambda x: x.split(' '))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_word2vec = open('embeddings/new_vector_SBW.txt', 'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('embeddings/SBW-vectors-300-min5.txt') as wordvecs:\n",
    "    for line in wordvecs:\n",
    "        word = line.split(' ')\n",
    "        word, features = word[0], word[1:]\n",
    "        if word in distribution_words:\n",
    "            new_word2vec.write(word + ' ' + ' '.join(features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_word2vec.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargando nuevamente los vectores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KeyedVectors.load_word2vec_format('embeddings/new_vector_SBW.txt', binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('inn', 0.5931442975997925),\n",
       " ('galta', 0.5310695171356201),\n",
       " ('fikk', 0.5252362489700317),\n",
       " ('riki', 0.5229285359382629),\n",
       " ('jag', 0.5188263654708862),\n",
       " ('sista', 0.5183577537536621),\n",
       " ('jah', 0.5087394714355469),\n",
       " ('yax', 0.5061405897140503),\n",
       " ('findes', 0.5053848028182983),\n",
       " ('dane', 0.5039175748825073)]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('ok')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corpus chat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cargando el corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usaremos nuestro corpus para entrenar nuestro word2vec con los siguientes parametros:\n",
    "\n",
    "  - Minima frecuencia de palabra: 5\n",
    "  - Negative samples=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Guardando el modelo entrenado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(sentences=df_cleaned.map(lambda x: x.split()), min_count=5, negative=20, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('embeddings/chat.w2v')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cargando el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec.load('embeddings/chat.w2v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('dale', 0.8783441781997681),\n",
       " ('üëçüèª', 0.7730407118797302),\n",
       " ('listo', 0.7725571990013123),\n",
       " ('gobbi', 0.7691001892089844),\n",
       " ('üò¨', 0.7633653879165649),\n",
       " ('dani', 0.7403482794761658),\n",
       " ('buenisimo', 0.7393252849578857),\n",
       " ('üòÅ', 0.7374236583709717),\n",
       " ('avisame', 0.7335799932479858),\n",
       " ('naho', 0.6965444087982178)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('ok')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pero como vectorizamos un contexto??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La primera opci√≥n fue sumar todos los vectores de palabras en un texto y as√≠ ten√≠amos una representaci√≥n del todo el contexto.\n",
    "\n",
    "Claramente aparec√≠a un gran problema...\n",
    "- Que pasa si los contextos son similares pero tienen una longitud distinta... entonces iban a estar en espacios diferentes :o\n",
    "\n",
    "La simple soluci√≥n fue promediarlo con la longitud del contexto y perfecto\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def context_to_vect(context):\n",
    "    return np.array([\n",
    "        np.mean([word2vec.get_vector(w)\n",
    "                 for w in context if w in word2vec.get_vocab()]\n",
    "                or [np.zeros(self.word2vec.get_dim())], axis=0)\n",
    "        for context in df_cleaned.map(lambda x: x.split())\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56778"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ctx2vec(context):\n",
    "    return np.array([\n",
    "        np.mean([model.wv[w]\n",
    "                 for w in context if w in model.wv]\n",
    "                or [np.zeros(model.vector_size)], axis=0)\n",
    "        for context in df_cleaned.map(lambda x: x.split())\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56778"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx = ctx2vec(df_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "filename = \"Xmatrix.pickle\"\n",
    "fileObj = open(filename, 'wb')\n",
    "pickle.dump(ctx, fileObj)\n",
    "fileObj.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 'MEDIA. lee esta historia! . una mujer muy enferma sono que jesus le dio a beber agua.. . cuando se desperto por la manana estaba bien otra vez, en forma, bien y curada.. . ella vio un trozo de papel al lado de su mesa que decia: \"jesus es el verdadero dios viviente\".. . le dijo a la gente acerca de lo que le habia sucedido a ella.. . un oficial que escucho la historia de la dama,  y al instante fue promovido en el trabajo.. . otro hombre que recibio el texto elimino el mensaje de inmediato y este  sufrio grandes perdidas durante NUM . veras como jesus hara maravilla yo si lo creo porque dios existe... ¬°amen!. . \"hola soy jesucristo, se que casi no tienes tiempo para mi... te amo y te bendigo siempre estoy contigo. hoy quiero que  este mensaje recorra todo el mundo antes de la media noche por favor no la cortes y te voy a ayudar con algo que tu estas necesitando... amen. . manda este mensaje y si no necesitas que dios te abra puertas, solo borralo.... . asi como cae agua del cielo,. üí¶üí¶üí¶üí¶üí¶üí¶üí¶üí¶üí¶. üí¶üí¶üí¶üí¶üí¶üí¶üí¶üí¶üí¶. üí¶üí¶üí¶üí¶üí¶üí¶üí¶üí¶üí¶. üí¶___ œÄ__üí¶üí¶üí¶üí¶üí¶üí¶.  /_______c. /\\\\ |  . |ÔΩú  Áî∞|ÈñÄ| œÄ||. | | |________|||___|üçÉüå∫üçÉ.  üåøüêêüåø üêêüåøüêêüåø. asi caeran en ti las mas bellas bendiciones en el nombre de jesus. (toma NUM y envialo). repite! jesus eres mi fuerza te amo, te necesito, saname y sana a mi familia pasalo a NUM y veras NUM esta.  noche. hola dani!vos sabes a donde venden fotocopiadoras usadas. le pregunto a mi jefe',\n",
       "       'gracias!!!!para cuando sabes?. üíÉ. le pregunto ahora a ver si sabe algo :p',\n",
       "       'üëèüëèüëèüëèüëè. ‚ú≥whatsapp le comunica, . que debido a la cantidad de mensajes y notificaciones que usted ha  enviado este mes, se le obsequiara:. un trapo de piso y una escoba, para que se ponga a hacer algo util !!. . ja. pasalo!!!',\n",
       "       ..., 'MEDIA. MEDIA', 'ü§∑\\u200d‚ôÄ',\n",
       "       'MEDIA. MEDIA. MEDIA. MEDIA. MEDIA. MEDIA. MEDIA. MEDIA. MEDIA. MEDIA. MEDIA. MEDIA. MEDIA'], dtype=object)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cleaned.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
