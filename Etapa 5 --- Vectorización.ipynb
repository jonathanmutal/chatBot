{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Formas de caracterizar en contexto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Haremos los siguientes experimentos para ver la mejor forma de caracterizar el contexto:\n",
    "    - bolsa de palabras (unigramas, bigramas, subwords, con y sin stopwords) \n",
    "    - Agregado por mi: tfidf con normalización para poner algunos pesos en las palabras\n",
    "    - si hay interacción del usuario objetivo y cómo se representa\n",
    "    - si hay respuestas\n",
    "    - si hay multimedia y de qué tipo\n",
    "__COMPLETAR__\n",
    "\n",
    "__PREGUNTAR:\n",
    "si hay interacción del usuario objetivo y cómo se representa??. si hay respuestas??__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usaremos diferentes word embeddings, obtenidos de diferentes tipos de corpus: genéricos o específicos de lenguaje generado por usuario o específicos del corpus del que estamos aprendiendo.\n",
    "    - Es decir, tres diferentes tipos de word embeddings:\n",
    "      - SBWCE\n",
    "      - con el mismo corpus de chats\n",
    "      - con un corpus de twitter\n",
    "      \n",
    "__BUSCAR CORPUS DE TWITTER__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cargando los datos de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('turn1_short1_pre2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Acordandonos de los datos.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idchat</th>\n",
       "      <th>context</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>✳ whatsapp le comunica que debido a la cantida...</td>\n",
       "      <td>ja</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>ok !</td>\n",
       "      <td>😘</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>para mi amiga hermana 💖 un ave maria hoy inici...</td>\n",
       "      <td>😘</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>en la parte de tu hermana</td>\n",
       "      <td>v</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>en el bautismo ahi eres puro pero si tu vuelve...</td>\n",
       "      <td>ok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>de que me perdi ? v</td>\n",
       "      <td>ja</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2</td>\n",
       "      <td>despues de bautizarte deberas apartarte de tod...</td>\n",
       "      <td>si</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>eso es fanatismo</td>\n",
       "      <td>si</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2</td>\n",
       "      <td>de que podemos hablar</td>\n",
       "      <td>😂</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2</td>\n",
       "      <td>no es el mismo por que para eso existen los bi...</td>\n",
       "      <td>hola</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   idchat                                            context label\n",
       "0       1  ✳ whatsapp le comunica que debido a la cantida...    ja\n",
       "1       1                                               ok !     😘\n",
       "2       1  para mi amiga hermana 💖 un ave maria hoy inici...     😘\n",
       "3       2                          en la parte de tu hermana     v\n",
       "4       2  en el bautismo ahi eres puro pero si tu vuelve...    ok\n",
       "5       2                                de que me perdi ? v    ja\n",
       "6       2  despues de bautizarte deberas apartarte de tod...    si\n",
       "7       2                                   eso es fanatismo    si\n",
       "8       2                              de que podemos hablar     😂\n",
       "9       2  no es el mismo por que para eso existen los bi...  hola"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorizando"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BOG: 1-gramas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(min_df=10, ngram_range=(1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "representation = vectorizer.fit(df.context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entonces obtenemos alrededor de tantas features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abajo', 'abuela', 'aca', 'acaba', 'acabo', 'acuerdo', 'ademas', 'adentro', 'adjuntado', 'afuera', 'agregar', 'agrego', 'agua', 'aguante', 'agustin', 'ah', 'ahh', 'ahi', 'ahora', 'aitor', 'aja', 'ajja', 'al', 'algebra', 'algo', 'alguien', 'algun', 'alguna', 'algunas', 'alguno', 'algunos', 'alianza', 'alla', 'alta', 'alto', 'ami', 'amiga', 'amigas', 'amigo', 'amigos', 'amo', 'amor', 'analisis', 'anda', 'andar', 'ando', 'ano', 'anoche', 'anos', 'antes', 'antorchas', 'anubis', 'aparece', 'aparte', 'apenas', 'aprender', 'aqui', 'archivo', 'argentina', 'arreglo', 'arriba', 'articulo', 'as', 'asi', 'asique', 'ataques', 'atras', 'audio', 'aula', 'aun', 'aunque', 'auto', 'avisame', 'avisen', 'aviso', 'ay', 'ayer', 'ayuda', 'baja', 'baje', 'bajo', 'banda', 'base', 'basta', 'bastante', 'be', 'bebe', 'bien', 'birra', 'bldo', 'bola', 'boludo', 'bosta', 'bue', 'buen', 'buena', 'buenas', 'buenisimo', 'bueno', 'buenos']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "909"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(representation.get_feature_names()[:100])\n",
    "len(representation.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOG = representation.transform(df.context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 24)\t1\n",
      "  (0, 186)\t2\n",
      "  (0, 281)\t1\n",
      "  (0, 346)\t1\n",
      "  (0, 357)\t1\n",
      "  (0, 407)\t1\n",
      "  (0, 431)\t1\n",
      "  (0, 437)\t2\n",
      "  (0, 502)\t1\n",
      "  (0, 587)\t1\n",
      "  (0, 676)\t3\n",
      "  (0, 728)\t2\n",
      "  (0, 838)\t1\n",
      "  (0, 839)\t1\n",
      "  (0, 898)\t1\n",
      "  (1, 567)\t1\n",
      "  (2, 18)\t2\n",
      "  (2, 24)\t1\n",
      "  (2, 36)\t2\n",
      "  (2, 41)\t2\n",
      "  (2, 154)\t1\n",
      "  (2, 164)\t1\n",
      "  (2, 178)\t2\n",
      "  (2, 186)\t8\n",
      "  (2, 187)\t1\n",
      "  :\t:\n",
      "  (2, 813)\t1\n",
      "  (2, 830)\t2\n",
      "  (2, 838)\t3\n",
      "  (3, 186)\t1\n",
      "  (3, 246)\t1\n",
      "  (3, 374)\t1\n",
      "  (3, 431)\t1\n",
      "  (3, 591)\t1\n",
      "  (3, 830)\t1\n",
      "  (4, 17)\t1\n",
      "  (4, 186)\t2\n",
      "  (4, 225)\t1\n",
      "  (4, 241)\t1\n",
      "  (4, 246)\t1\n",
      "  (4, 260)\t1\n",
      "  (4, 431)\t1\n",
      "  (4, 458)\t1\n",
      "  (4, 516)\t1\n",
      "  (4, 575)\t1\n",
      "  (4, 618)\t1\n",
      "  (4, 728)\t1\n",
      "  (4, 743)\t1\n",
      "  (4, 800)\t1\n",
      "  (4, 830)\t1\n",
      "  (4, 874)\t1\n"
     ]
    }
   ],
   "source": [
    "print(BOG[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import save_npz\n",
    "\n",
    "save_npz(\"BOG1.npz\", BOG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BOG: 2-gramas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=10,\n",
       "        ngram_range=(1, 2),\n",
       "        preprocessor=<function clean_text_1 at 0x7ff73bdf0950>,\n",
       "        stop_words=None, strip_accents=None,\n",
       "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "representation.set_params(ngram_range=(1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14401"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BOG = representation.fit(df[1:].context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BOG: 3-gramas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=10,\n",
       "        ngram_range=(1, 3),\n",
       "        preprocessor=<function clean_text_1 at 0x7fafab945c80>,\n",
       "        stop_words=None, strip_accents=None,\n",
       "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "representation.set_params(ngram_range=(1, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17048"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(representation.fit(df[1:].context).get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOG_matrix = representation.transform(df[1:].context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(56778, 17048)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BOG_matrix.get_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'chicos. una pregunta. cómo se escribe la dirección postal en vuestras ciudades?. es que los otros días vi una de francia y es un poco diferente a como yo escribo la mía. La de aca por donde vivo es 1010. El cod postal. así que si pongo n un sobre 1010 te llega?. Pero es por zona, no se si s igual por alla. qué raro sois los venezolanos. no?. No, obvio pones la direccion😂😂'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.context[28]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ NUM: 2, aca: 1, aca por: 1, alla: 1, asi: 1, asi que: 1, asi que si: 1, chicos: 1, chicos una: 1, ciudades: 1, cod: 1, como: 2, como se: 1, como yo: 1, de: 2, de aca: 1, dias: 1, diferente: 1, direccion: 2, donde: 1, en: 1, es: 4, es NUM: 1, es por: 1, es que: 1, es un: 1, es un poco: 1, escribe: 1, escribo: 1, igual: 1, igual por: 1, la: 4, la de: 1, la direccion: 2, la mia: 1, llega: 1, los: 2, los otros: 1, mia: 1, no: 3, no no: 1, no se: 1, no se si: 1, obvio: 1, otros: 1, pero: 1, pero es: 1, poco: 1, pones: 1, pongo: 1, por: 3, por alla: 1, por donde: 1, pregunta: 1, que: 3, que los: 1, que raro: 1, que si: 1, raro: 1, se: 2, se escribe: 1, se si: 1, si: 2, si igual: 1, sobre: 1, un: 2, un poco: 1, una: 2, una de: 1, una pregunta: 1, vi: 1, vivo: 1, yo: 1, zona: 1, }"
     ]
    }
   ],
   "source": [
    "arr_repr = BOG_matrix[27].toarray()[0]\n",
    "print('{ ', end='')\n",
    "for i in BOG_matrix[27].indices:\n",
    "    print('{0}: {1}'.format(representation.get_feature_names()[i], arr_repr[i]), end=', ')\n",
    "print('}', end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BOG: (3,4)-Subwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(min_df=10, analyzer='char', ngram_range=(3,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='char', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=10,\n",
       "        ngram_range=(3, 4),\n",
       "        preprocessor=<function clean_text_1 at 0x7fafab945c80>,\n",
       "        stop_words='!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~¡',\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "representation = vectorizer.fit(df[1:].context)\n",
    "BOG_matrix = representation.transform(df[1:].context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24863"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(representation.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'chicos. una pregunta. cómo se escribe la dirección postal en vuestras ciudades?. es que los otros días vi una de francia y es un poco diferente a como yo escribo la mía. La de aca por donde vivo es 1010. El cod postal. así que si pongo n un sobre 1010 te llega?. Pero es por zona, no se si s igual por alla. qué raro sois los venezolanos. no?. No, obvio pones la direccion😂😂'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.context[28]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " NU: 2,  NUM: 2,  a : 1,  a c: 1,  ac: 1,  aca: 1,  al: 1,  all: 1,  as: 1,  asi: 1,  ci: 1,  ciu: 1,  co: 3,  cod: 1,  com: 2,  de: 2,  de : 2,  di: 4,  dia: 1,  dif: 1,  dir: 2,  do: 1,  don: 1,  en: 1,  en : 1,  es: 6,  es : 4,  esc: 2,  fr: 1,  fra: 1,  ig: 1,  igu: 1,  la: 4,  la : 4,  ll: 1,  lle: 1,  lo: 2,  los: 2,  mi: 1,  mia: 1,  n : 1,  no: 3,  no : 3,  ob: 1,  obv: 1,  ot: 1,  otr: 1,  pe: 1,  per: 1,  po: 8,  poc: 1,  pon: 2,  por: 3,  pos: 2,  pr: 1,  pre: 1,  qu: 3,  que: 3,  ra: 1,  rar: 1,  s : 1,  se: 2,  se : 2,  si: 2,  si : 2,  so: 2,  sob: 1,  un: 4,  un : 2,  una: 2,  ve: 1,  ven: 1,  vi: 2,  vi : 1,  viv: 1,  vu: 1,  vue: 1,  y : 1,  y e: 1,  yo: 1,  yo : 1,  zo: 1,  zon: 1, M c: 1, M co: 1, M l: 1, M ll: 1, NUM: 2, NUM : 2, UM : 2, UM c: 1, UM l: 1, a c: 2, a co: 2, a d: 4, a de: 2, a di: 2, a l: 1, a la: 1, a m: 1, a mi: 1, a n: 1, a no: 1, a p: 3, a pe: 1, a po: 1, a pr: 1, a q: 1, a qu: 1, a y: 1, a y : 1, aca: 1, aca : 1, ade: 1, ades: 1, al : 3, al a: 1, al e: 1, al p: 1, all: 1, alla: 1, anc: 1, anci: 1, ano: 1, anos: 1, aro: 1, aro : 1, as : 2, as c: 1, as v: 1, asi: 1, asi : 1, be : 1, be l: 1, bo : 1, bo l: 1, bre: 1, bre : 1, bvi: 1, bvio: 1, ca : 1, ca p: 1, cci: 2, ccio: 2, chi: 1, chic: 1, cia: 1, cia : 1, cio: 2, cion: 2, ciu: 1, ciud: 1, co : 1, co d: 1, cod: 1, cod : 1, com: 2, como: 2, cos: 1, cos : 1, cri: 2, crib: 2, d p: 1, d po: 1, dad: 1, dade: 1, de : 3, de a: 1, de f: 1, de v: 1, des: 1, des : 1, dia: 1, dias: 1, dif: 1, dife: 1, dir: 2, dire: 2, don: 1, dond: 1, e N: 1, e NU: 1, e a: 2, e a : 1, e ac: 1, e e: 1, e es: 1, e f: 1, e fr: 1, e l: 2, e la: 1, e lo: 1, e r: 1, e ra: 1, e s: 2, e si: 2, e v: 1, e vi: 1, ecc: 2, ecci: 2, ega: 1, ega : 1, egu: 1, egun: 1, en : 1, en v: 1, ene: 1, enez: 1, ent: 1, ente: 1, ere: 1, eren: 1, ero: 1, ero : 1, es : 6, es N: 1, es e: 1, es l: 1, es p: 1, es q: 1, es u: 1, esc: 2, escr: 2, est: 1, estr: 1, ezo: 1, fer: 1, fere: 1, fra: 1, fran: 1, ga : 1, ga p: 1, go : 1, go n: 1, gua: 1, gual: 1, gun: 1, gunt: 1, hic: 1, hico: 1, i p: 1, i po: 1, i q: 1, i qu: 1, i s: 1, i u: 1, i un: 1, ia : 2, ia l: 1, ia y: 1, ias: 1, ias : 1, ibe: 1, ibe : 1, ibo: 1, ibo : 1, ico: 1, icos: 1, ife: 1, ifer: 1, igu: 1, igua: 1, io : 1, io p: 1, ion: 2, ion : 1, ire: 2, irec: 2, is : 1, is l: 1, iud: 1, iuda: 1, ivo: 1, ivo : 1, l a: 1, l as: 1, l e: 1, l en: 1, l p: 1, l po: 1, la : 5, la d: 3, la m: 1, la q: 1, lan: 1, lano: 1, leg: 1, lega: 1, lla: 1, lla : 1, lle: 1, lleg: 1, los: 2, los : 2, mia: 1, mia : 1, mo : 2, mo s: 1, mo y: 1, n p: 2, n po: 2, n s: 1, n so: 1, n u: 1, n un: 1, n v: 1, n vu: 1, na : 3, na d: 1, na n: 1, na p: 1, nci: 1, ncia: 1, nde: 1, nde : 1, nes: 1, nes : 1, nez: 1, ngo: 1, ngo : 1, no : 3, no n: 1, no o: 1, no s: 1, nos: 1, nos : 1, nta: 1, nta : 1, nte: 1, nte : 1, o d: 1, o di: 1, o e: 3, o es: 3, o l: 1, o la: 1, o n: 2, o n : 1, o no: 1, o o: 1, o ob: 1, o p: 1, o po: 1, o s: 3, o se: 2, o so: 1, o y: 1, o yo: 1, obr: 1, obre: 1, obv: 1, obvi: 1, oco: 1, oco : 1, od : 1, od p: 1, ois: 1, ois : 1, ola: 1, olan: 1, omo: 2, omo : 2, on : 1, on p: 1, ona: 1, ona : 1, ond: 1, onde: 1, one: 1, ones: 1, ong: 1, ongo: 1, or : 3, or a: 1, or d: 1, os : 5, os d: 1, os n: 1, os o: 1, os u: 1, os v: 1, ost: 2, osta: 2, otr: 1, otro: 1, per: 1, pero: 1, poc: 1, poco: 1, pon: 2, pone: 1, pong: 1, por: 3, por : 3, pos: 2, post: 2, pre: 1, preg: 1, que: 3, que : 3, r a: 1, r al: 1, r d: 1, r do: 1, r z: 1, ran: 1, ranc: 1, rar: 1, raro: 1, ras: 1, ras : 1, re : 1, re N: 1, rec: 2, recc: 2, reg: 1, regu: 1, ren: 1, rent: 1, rib: 2, ribe: 1, ribo: 1, ro : 2, ro e: 1, ro s: 1, ros: 1, ros : 1, s N: 1, s NU: 1, s c: 1, s ci: 1, s d: 1, s di: 1, s e: 1, s es: 1, s i: 1, s ig: 1, s l: 2, s la: 1, s lo: 1, s n: 1, s no: 1, s o: 1, s ot: 1, s p: 1, s po: 1, s q: 1, s qu: 1, s u: 2, s un: 2, s v: 2, s ve: 1, s vi: 1, scr: 2, scri: 2, se : 2, se e: 1, se s: 1, si : 3, si p: 1, si q: 1, si s: 1, sob: 1, sobr: 1, soi: 1, sta: 2, stal: 2, str: 1, stra: 1, ta : 1, ta c: 1, tal: 2, tal : 2, te : 1, te a: 1, tra: 1, tras: 1, tro: 1, tros: 1, ual: 1, ual : 1, uda: 1, udad: 1, ue : 3, ue l: 1, ue r: 1, ue s: 1, ues: 1, uest: 1, un : 2, un p: 1, un s: 1, una: 2, una : 2, unt: 1, unta: 1, ven: 1, vene: 1, vi : 1, vi u: 1, vio: 1, vio : 1, viv: 1, vivo: 1, vo : 1, vo e: 1, vue: 1, y e: 1, y es: 1, yo : 1, yo e: 1, zon: 1, zona: 1, "
     ]
    }
   ],
   "source": [
    "arr_repr = BOG_matrix[27].toarray()[0]\n",
    "for i in BOG_matrix[27].indices:\n",
    "    print('{0}: {1}'.format(representation.get_feature_names()[i], arr_repr[i]), end=', ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFidf: 1-gramas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(min_df=10, ngram_range=(1, 1), norm='l2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "representation = vectorizer.fit(df[1:].context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOG_matrix = representation.transform(df[1:].context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entonces podemos decir que la frase:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'😘. Hola Isa Lamento lo de tu mamá 😥 Que descanse en paz'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.context[7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta representada como:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 4610)\t0.444007229186\n",
      "  (0, 4608)\t0.232292394347\n",
      "  (0, 3939)\t0.285705626552\n",
      "  (0, 3473)\t0.497750374311\n",
      "  (0, 3381)\t0.647699532027\n"
     ]
    }
   ],
   "source": [
    "print(BOG_matrix[6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Donde los features son..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('saca', 'sabor', 'peso', 'necesiten', 'moral')"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "representation.get_feature_names()[4449], representation.get_feature_names()[4447], representation.get_feature_names()[3802], representation.get_feature_names()[3348], representation.get_feature_names()[3261]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFidf: 3-gramas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(min_df=10, ngram_range=(1, 3), norm='l2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "representation = vectorizer.fit(df[1:].context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOG_matrix = representation.transform(df[1:].context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 14962)\t0.251093106197\n",
      "  (0, 10991)\t0.526693785555\n",
      "  (0, 7090)\t0.708823214338\n",
      "  (0, 6946)\t0.34980486296\n",
      "  (0, 3044)\t0.186419196821\n",
      "  (1, 13416)\t0.445651009621\n",
      "  (1, 12802)\t0.260870206023\n",
      "  (1, 12700)\t0.114103432201\n",
      "  (1, 9129)\t0.312658204155\n",
      "  (1, 6600)\t0.238068586996\n",
      "  (1, 5707)\t0.481824210078\n",
      "  (1, 5161)\t0.445651009621\n",
      "  (1, 5101)\t0.145516231116\n",
      "  (1, 1117)\t0.337573917747\n",
      "  (2, 16328)\t0.45418585109\n",
      "  (2, 11596)\t0.520149510929\n",
      "  (2, 7360)\t0.463337136898\n",
      "  (2, 6171)\t0.360458478114\n",
      "  (2, 1504)\t0.422549502516\n",
      "  (3, 10995)\t0.391758851566\n",
      "  (3, 10264)\t0.471306759228\n",
      "  (3, 7671)\t0.549778867213\n",
      "  (3, 4420)\t0.514157183574\n",
      "  (3, 4180)\t0.175496199971\n",
      "  (3, 3044)\t0.164260806241\n",
      "  (6, 14494)\t0.354404835023\n",
      "  (6, 14225)\t0.185414881299\n",
      "  (6, 12162)\t0.22804911449\n",
      "  (6, 11046)\t0.602397708492\n",
      "  (6, 11040)\t0.397302403417\n",
      "  (6, 10463)\t0.51699123506\n",
      "  (7, 13366)\t1.0\n",
      "  (8, 16875)\t1.0\n"
     ]
    }
   ],
   "source": [
    "print(BOG_matrix[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entonces podemos decir que la frase:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'chicos. una pregunta. cómo se escribe la dirección postal en vuestras ciudades?. es que los otros días vi una de francia y es un poco diferente a como yo escribo la mía. La de aca por donde vivo es 1010. El cod postal. así que si pongo n un sobre 1010 te llega?. Pero es por zona, no se si s igual por alla. qué raro sois los venezolanos. no?. No, obvio pones la direccion😂😂'"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.context[28]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta representada como:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 17044)\t0.122346591758\n",
      "  (0, 16875)\t0.0505344426973\n",
      "  (0, 16562)\t0.11856716976\n",
      "  (0, 16448)\t0.0876481532616\n",
      "  (0, 16023)\t0.114368515018\n",
      "  (0, 15958)\t0.111328285247\n",
      "  (0, 15924)\t0.109401534642\n",
      "  (0, 15876)\t0.09219704563\n",
      "  (0, 15780)\t0.10054062254\n",
      "  (0, 14590)\t0.0924285941421\n",
      "  (0, 14324)\t0.128110828987\n",
      "  (0, 14225)\t0.0864893830059\n",
      "  (0, 14003)\t0.092006694011\n",
      "  (0, 13890)\t0.139659678239\n",
      "  (0, 13857)\t0.0964611674483\n",
      "  (0, 13438)\t0.10145416588\n",
      "  (0, 13135)\t0.0891153203862\n",
      "  (0, 13097)\t0.123745147403\n",
      "  (0, 12956)\t0.104691433598\n",
      "  (0, 12700)\t0.108599777491\n",
      "  (0, 12378)\t0.0970086224212\n",
      "  (0, 12181)\t0.11697464841\n",
      "  (0, 12173)\t0.142325670723\n",
      "  (0, 12162)\t0.159565082391\n",
      "  (0, 12151)\t0.0983157920864\n",
      "  :\t:\n",
      "  (0, 5245)\t0.0841350833944\n",
      "  (0, 5237)\t0.113895987251\n",
      "  (0, 5102)\t0.11420940119\n",
      "  (0, 5101)\t0.184663219042\n",
      "  (0, 4632)\t0.043493175501\n",
      "  (0, 4027)\t0.0763201425025\n",
      "  (0, 3957)\t0.242623955266\n",
      "  (0, 3875)\t0.128906565576\n",
      "  (0, 3822)\t0.0942850049113\n",
      "  (0, 3058)\t0.120578921785\n",
      "  (0, 3044)\t0.0748627773091\n",
      "  (0, 2302)\t0.131095183094\n",
      "  (0, 2277)\t0.100379210455\n",
      "  (0, 2216)\t0.111830969225\n",
      "  (0, 2157)\t0.13886394165\n",
      "  (0, 2092)\t0.145546876183\n",
      "  (0, 2038)\t0.142325670723\n",
      "  (0, 2015)\t0.0806051631679\n",
      "  (0, 1217)\t0.136695750024\n",
      "  (0, 1212)\t0.0879948372376\n",
      "  (0, 1181)\t0.0641662197014\n",
      "  (0, 924)\t0.0966983428321\n",
      "  (0, 404)\t0.145546876183\n",
      "  (0, 389)\t0.0818514394602\n",
      "  (0, 158)\t0.0912586035071\n"
     ]
    }
   ],
   "source": [
    "print(BOG_matrix[27])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Donde los features son..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zona: 0.12234659175774053, yo: 0.050534442697347366, vivo: 0.11856716975995497, vi: 0.0876481532615862, una pregunta: 0.1143685150183527, una de: 0.11132828524748828, una: 0.10940153464161766, un poco: 0.09219704562998841, un: 0.10054062254010157, sobre: 0.09242859414211325, si igual: 0.1281108289871265, si: 0.0864893830059329, se si: 0.09200669401099533, se escribe: 0.13965967823906839, se: 0.09646116744828899, raro: 0.1014541658802468, que si: 0.0891153203862292, que raro: 0.12374514740338534, que los: 0.10469143359780607, que: 0.10859977749099453, pregunta: 0.09700862242124296, por donde: 0.11697464840972999, por alla: 0.14232567072328067, por: 0.15956508239073838, pongo: 0.09831579208638877, pones: 0.11587912544216117, poco: 0.085767058832565, pero es: 0.10377616506189986, pero: 0.05238518845298545, otros: 0.09759410557706343, obvio: 0.09968114551481815, no se si: 0.09274280158788244, no se: 0.06596438580282649, no no: 0.09500251833050471, no: 0.11351884900093571, mia: 0.11132828524748828, los otros: 0.11420940118955161, los: 0.10530462068028322, llega: 0.11199996861179086, la mia: 0.12182066131348711, la direccion: 0.264130180590944, la de: 0.10057517258490781, la: 0.15812530743968584, igual por: 0.13965967823906839, igual: 0.07116729752801136, escribo: 0.11502180995925212, escribe: 0.12345516770236246, es un poco: 0.14049844803858233, es un: 0.08643941409042437, es que: 0.08413508339439471, es por: 0.11389598725097232, es NUM: 0.11420940118955161, es: 0.184663219042431, en: 0.0434931755009734, donde: 0.07632014250248814, direccion: 0.24262395526578306, diferente: 0.1289065655764879, dias: 0.09428500491126841, de aca: 0.12057892178460922, de: 0.07486277730914422, como yo: 0.1310951830936827, como se: 0.10037921045534302, como: 0.111830969225467, cod: 0.13886394164970695, ciudades: 0.1455468761833439, chicos una: 0.14232567072328067, chicos: 0.08060516316789397, asi que si: 0.13669575002395862, asi que: 0.087994837237629, asi: 0.06416621970135543, alla: 0.09669834283206566, aca por: 0.1455468761833439, aca: 0.08185143946016502, NUM: 0.09125860350705094, "
     ]
    }
   ],
   "source": [
    "dict_repr = dict()\n",
    "arr_repr = BOG_matrix[27].toarray()[0]\n",
    "for i in BOG_matrix[27].indices:\n",
    "    print('{0}: {1}'.format(representation.get_feature_names()[i], arr_repr[i]), end=', ')\n",
    "    dict_repr[representation.get_feature_names()[i]] = arr_repr[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Palabras más representativas para la frase:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('la direccion', 0.26413018059094401),\n",
       " ('direccion', 0.24262395526578306),\n",
       " ('es', 0.18466321904243099),\n",
       " ('por', 0.15956508239073838),\n",
       " ('la', 0.15812530743968584),\n",
       " ('ciudades', 0.14554687618334389),\n",
       " ('aca por', 0.14554687618334389),\n",
       " ('por alla', 0.14232567072328067),\n",
       " ('chicos una', 0.14232567072328067),\n",
       " ('es un poco', 0.14049844803858233)]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(dict_repr.items(), key=lambda x: x[1], reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "filename = \"BOGmatrix.pickle\"\n",
    "fileObj = open(filename, 'wb')\n",
    "pickle.dump(ctx, fileObj)\n",
    "fileObj.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pues claramente esta pidiendo una dirección"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tomamos algunos embeddings de:\n",
    "- [SBWCE](http://crscardellino.me/SBWCE/) --> 300 dimensiones\n",
    "- Nuestro corupus --> 100 dimensiones\n",
    "\n",
    "**TWITER???**\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SBWCE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Empezaremos cargando los vectores pre-entrenados con el algoritmo de Word2Vec. Para ello usaremos Gensim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KeyedVectors.load_word2vec_format('embeddings/SBW-vectors-300-min5.bin.gz', binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De esta forma no nos alcanza la memoria (por ahora), por lo que vamos a filtrar las palabras que usaremos y guardaremos un nuevo archivo más pequeño."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Achicando el vocabulario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "distribution_words = Counter(chain.from_iterable(df_cleaned.map(lambda x: x.split(' '))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_word2vec = open('embeddings/new_vector_SBW.txt', 'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('embeddings/SBW-vectors-300-min5.txt') as wordvecs:\n",
    "    for line in wordvecs:\n",
    "        word = line.split(' ')\n",
    "        word, features = word[0], word[1:]\n",
    "        if word in distribution_words:\n",
    "            new_word2vec.write(word + ' ' + ' '.join(features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_word2vec.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargando nuevamente los vectores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KeyedVectors.load_word2vec_format('embeddings/new_vector_SBW.txt', binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('inn', 0.5931442975997925),\n",
       " ('galta', 0.5310695171356201),\n",
       " ('fikk', 0.5252362489700317),\n",
       " ('riki', 0.5229285359382629),\n",
       " ('jag', 0.5188263654708862),\n",
       " ('sista', 0.5183577537536621),\n",
       " ('jah', 0.5087394714355469),\n",
       " ('yax', 0.5061405897140503),\n",
       " ('findes', 0.5053848028182983),\n",
       " ('dane', 0.5039175748825073)]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('ok')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corpus chat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cargando el corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usaremos nuestro corpus para entrenar nuestro word2vec con los siguientes parametros:\n",
    "\n",
    "  - Minima frecuencia de palabra: 5\n",
    "  - Negative samples=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Guardando el modelo entrenado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(sentences=df_cleaned.map(lambda x: x.split()), min_count=5, negative=20, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('embeddings/chat.w2v')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cargando el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec.load('embeddings/chat.w2v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('dale', 0.8783441781997681),\n",
       " ('👍🏻', 0.7730407118797302),\n",
       " ('listo', 0.7725571990013123),\n",
       " ('gobbi', 0.7691001892089844),\n",
       " ('😬', 0.7633653879165649),\n",
       " ('dani', 0.7403482794761658),\n",
       " ('buenisimo', 0.7393252849578857),\n",
       " ('😁', 0.7374236583709717),\n",
       " ('avisame', 0.7335799932479858),\n",
       " ('naho', 0.6965444087982178)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('ok')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pero como vectorizamos un contexto??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La primera opción fue sumar todos los vectores de palabras en un texto y así teníamos una representación del todo el contexto.\n",
    "\n",
    "Claramente aparecía un gran problema...\n",
    "- Que pasa si los contextos son similares pero tienen una longitud distinta... entonces iban a estar en espacios diferentes :o\n",
    "\n",
    "La simple solución fue promediarlo con la longitud del contexto y perfecto\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def context_to_vect(context):\n",
    "    return np.array([\n",
    "        np.mean([word2vec.get_vector(w)\n",
    "                 for w in context if w in word2vec.get_vocab()]\n",
    "                or [np.zeros(self.word2vec.get_dim())], axis=0)\n",
    "        for context in df_cleaned.map(lambda x: x.split())\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56778"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ctx2vec(context):\n",
    "    return np.array([\n",
    "        np.mean([model.wv[w]\n",
    "                 for w in context if w in model.wv]\n",
    "                or [np.zeros(model.vector_size)], axis=0)\n",
    "        for context in df_cleaned.map(lambda x: x.split())\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56778"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx = ctx2vec(df_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "filename = \"Xmatrix.pickle\"\n",
    "fileObj = open(filename, 'wb')\n",
    "pickle.dump(ctx, fileObj)\n",
    "fileObj.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 'MEDIA. lee esta historia! . una mujer muy enferma sono que jesus le dio a beber agua.. . cuando se desperto por la manana estaba bien otra vez, en forma, bien y curada.. . ella vio un trozo de papel al lado de su mesa que decia: \"jesus es el verdadero dios viviente\".. . le dijo a la gente acerca de lo que le habia sucedido a ella.. . un oficial que escucho la historia de la dama,  y al instante fue promovido en el trabajo.. . otro hombre que recibio el texto elimino el mensaje de inmediato y este  sufrio grandes perdidas durante NUM . veras como jesus hara maravilla yo si lo creo porque dios existe... ¡amen!. . \"hola soy jesucristo, se que casi no tienes tiempo para mi... te amo y te bendigo siempre estoy contigo. hoy quiero que  este mensaje recorra todo el mundo antes de la media noche por favor no la cortes y te voy a ayudar con algo que tu estas necesitando... amen. . manda este mensaje y si no necesitas que dios te abra puertas, solo borralo.... . asi como cae agua del cielo,. 💦💦💦💦💦💦💦💦💦. 💦💦💦💦💦💦💦💦💦. 💦💦💦💦💦💦💦💦💦. 💦___ π__💦💦💦💦💦💦.  /_______c. /\\\\ |  . |｜  田|門| π||. | | |________|||___|🍃🌺🍃.  🌿🐐🌿 🐐🌿🐐🌿. asi caeran en ti las mas bellas bendiciones en el nombre de jesus. (toma NUM y envialo). repite! jesus eres mi fuerza te amo, te necesito, saname y sana a mi familia pasalo a NUM y veras NUM esta.  noche. hola dani!vos sabes a donde venden fotocopiadoras usadas. le pregunto a mi jefe',\n",
       "       'gracias!!!!para cuando sabes?. 💃. le pregunto ahora a ver si sabe algo :p',\n",
       "       '👏👏👏👏👏. ✳whatsapp le comunica, . que debido a la cantidad de mensajes y notificaciones que usted ha  enviado este mes, se le obsequiara:. un trapo de piso y una escoba, para que se ponga a hacer algo util !!. . ja. pasalo!!!',\n",
       "       ..., 'MEDIA. MEDIA', '🤷\\u200d♀',\n",
       "       'MEDIA. MEDIA. MEDIA. MEDIA. MEDIA. MEDIA. MEDIA. MEDIA. MEDIA. MEDIA. MEDIA. MEDIA. MEDIA'], dtype=object)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cleaned.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
